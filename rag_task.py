# -*- coding: utf-8 -*-
"""RAG-task.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XU_ocQuj4HKynFPfTIXR7qy3Gj5E6rIU

Installing Necessary Libraries
"""

!pip install pdfplumber
!pip install langchain
!pip install faiss-cpu
!pip install transformers
!pip install sentence-transformers
!pip install huggingface_hub
!pip install langchain-community

!pip install pdfplumber

"""Upload the document"""

from google.colab import files
uploaded = files.upload()

import pdfplumber

def extract_text_with_plumber(pdf_path):
    full_text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for i, page in enumerate(pdf.pages):
            text = page.extract_text()
            if text:
                full_text += f"\n\nPage {i + 1}\n{text.strip()}"
    return full_text

pdf_path = "/content/OJ_L_202401689_EN_TXT.pdf"  # Replace with your path
raw_text = extract_text_with_plumber(pdf_path)
print(raw_text[:1000])  # Preview first 1000 characters

"""CHUNK THE TEXT"""

from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=["\n\n", "\n", ".", " ", ""]
)

docs = text_splitter.create_documents([raw_text])

for i, doc in enumerate(docs):
    doc.metadata["source"] = f"Page {i + 1}"

print(f"Total Chunks: {len(docs)}")

"""Creating embeddings and using VB faiss"""

from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

db = FAISS.from_documents(docs, embedding_model)

db.save_local("faiss_index")

print(f"Total chunks stored in FAISS: {len(db.docstore._dict)}")

for i, doc in enumerate(list(db.docstore._dict.values())[:5]):
    print(f"\nðŸ”¹ Chunk {i+1}")
    print(f"Metadata: {doc.metadata}")
    print(f"Content preview: {doc.page_content[:300]}...")

from huggingface_hub import login
login(token="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")

from huggingface_hub import login
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.llms import HuggingFacePipeline

# Login using Hugging Face token
login(token="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")

#  Load model and tokenizer (Requires GPU)
model_id = "mistralai/Mistral-7B-Instruct-v0.1"

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    use_auth_token=True,
    torch_dtype="auto",  # Can be float16 if you're on Colab with GPU
    device_map="auto",   # Automatically maps to GPU if available
    trust_remote_code=True
)

# Create generation pipeline
gen_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=512,
    temperature=0.7,
    top_p=0.9,
    repetition_penalty=1.1
)

# Wrap in LangChain LLM
llm = HuggingFacePipeline(pipeline=gen_pipeline)

def get_similar_chunks(query, db, k=5):
    docs_and_scores = db.similarity_search_with_score(query, k=k)
    return [doc.page_content for doc, score in docs_and_scores]

import time
import re

def get_similar_chunks(query, db, k=8):
    docs_and_scores = db.similarity_search_with_score(query, k=k)
    return docs_and_scores

def build_prompt(context_chunks, query):
    context = "\n\n".join([doc.page_content for doc, score in context_chunks])

    return f"""
You are a legal assistant AI. Your task is to answer the user's question using **only** the legal content in the context below.

**IMPORTANT:** You must not use any external sources or prior knowledge, even if the answer seems obvious. Use only the information explicitly present in the context.

Context:
{context}

Question: {query}

Instructions:
- Use only facts, language, and terminology that are explicitly written in the context.
- Do not include information from outside the provided context.
- Do not summarize or paraphrase unless explicitly supported by the wording in the context.
- Reuse legal phrases, article names, or citations **exactly** as they appear in the context.
- Cite every factual claim using the format (see [Page X]) or (see Article X), depending on the metadata.
- If the context does not provide a full answer, write: "The context does not provide a complete answer to this question." Then include only what is clearly written.
- Do not make abstract statements such as "ensures ethical use" unless those exact words appear in the context.
- If something is listed in the context, retain the same format (bullets, numbering, or wording).
- Use clear, complete English sentences in a formal tone.

Answer:
""".strip()




def fix_incomplete_sentence(text):
    sentences = re.split(r'(?<=[.?!])\s+', text.strip())
    if not sentences[-1].endswith(('.', '?', '!')):
        sentences = sentences[:-1]  # Remove incomplete sentence
    return ' '.join(sentences)

def ask_question(query, db, llm):
    start_time = time.time()
    retrieved = get_similar_chunks(query, db)
    prompt = build_prompt(retrieved, query)

    response = llm.invoke(prompt)
    response = fix_incomplete_sentence(response)  # Fix truncated output

    end_time = time.time()
    print(f"\n Question: {query}")
    print(f"\n")
    print(f"\n Answer:\n{response}")
    print(f"\n Generation Time: {round(end_time - start_time, 2)} seconds")

    return response, round(end_time - start_time, 2)

questions = [
    "Please explain what the EU AI Act is.",
    "How is the testing of high-risk AI systems performed in real world conditions?",
    "In what cases a company that develops AI solutions should obtain permission to deploy it?"
]

results = []

for q in questions:
    answer, gen_time = ask_question(q, db, llm)
    results.append((q, answer, gen_time))

with open("rag_outputs.txt", "w") as f:
    for q, ans, t in results:
        f.write(f"\nðŸ”¹ Question: {q}\n\nAnswer:\n{ans}\n\nTime: {t} sec\n{'-'*80}\n")

def evaluate_answer(context, question, answer, model, tokenizer, max_new_tokens=100):
    prompt = f"""You are an AI evaluation assistant. Read the context, question, and answer. Decide if the answer is correct and based only on the context.

Context:
{context}

Question:
{question}

Answer:
{answer}

Evaluation Task:
Is the answer correct and based only on the context? Reply "Yes" or "No" and give a short reason.
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=False,
        temperature=0.0,
        pad_token_id=tokenizer.eos_token_id
    )

    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Extract only the evaluation part (cut off prompt if needed)
    evaluation = decoded[len(prompt):].strip()
    return evaluation

context = """
10. Any Member States in which the AI system has been placed on the market, put into service or made available in the
Union;
11. A copy of the EU declaration of conformity referred to in Article 47;
12. Electronic instructions for use; this information shall not be provided for high-risk AI systems in the areas of law
enforcement or migration, asylum and border control management referred to in Annex III, points 1, 6 and 7;
13. A URL for additional information (optional). general-purpose AI models placed on the market or put into service in the Union meet the relevant requirements or
obligations laid down in this Regulation. The Commission shall request the European standardisation organisations to provide evidence of their best efforts to fulfil
the objectives referred to in the first and the second subparagraph of this paragraph in accordance with Article 24 of
Regulation (EU) No 1025/2012. examination procedure referred to in Article 98(2). 2. Before preparing a draft implementing act, the Commission shall inform the committee referred to in Article 22 of
Regulation (EU) No 1025/2012 that it considers the conditions laid down in paragraph 1 of this Article to be fulfilled. 3. High-risk AI systems or general-purpose AI models which are in conformity with the common specifications referred

2020/1828 (Artificial Intelligence Act) (OJ L, 2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/
2024/1689/oj).â€™. Article 111
AI systems already placed on the market or put into service and general-purpose AI models already placed on the
marked
1. Without prejudice to the application of Article 5 as referred to in Article 113(3), point (a), AI systems which are
components of the large-scale IT systems established by the legal acts listed in Annex X that have been placed on the market

for each high-risk AI system, and keep it at the disposal of the national competent authorities for 10 years after the
high-risk AI system has been placed on the market or put into service. The EU declaration of conformity shall identify the
high-risk AI system for which it has been drawn up. A copy of the EU declaration of conformity shall be submitted to the
relevant national competent authorities upon request.
"""

question = "Please explain what the EU AI Act is."

answer = """The EU AI Act is Regulation (EU) 2020/1828 on Artificial Intelligence Act (OJ L, 2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/2024/1689/oj). It lays down the requirements and obligations for the placing on the market, putting into service, or making available in the Union of AI systems, including general-purpose AI models. The Act aims to ensure that AI systems meet relevant requirements or obligations, such as being compliant with the common specifications referred to in the Act, being accompanied by electronic instructions for use, having a copy of the EU declaration of conformity, and being made available to national competent authorities for 10 years after being placed on the market or put into service."""

# Use the evaluation function defined earlier
result = evaluate_answer(context, question, answer, model, tokenizer)
print(result)

with open("requirements.txt", "w") as f:
    f.write("""pdfplumber
langchain
faiss-cpu
transformers
sentence-transformers
huggingface_hub
langchain-community
""")

!git config --global user.email "fatimaduaa053@gmail.com"
!git config --global user.name "Duaa Fatima"

!git init
!git remote add origin https://github.com/Duaa-Fatima/RAG-PIPELINE.git
!git add .
!git commit -m "Initial commit from Colab"
!git branch -M main
!git push -u origin main

